---
title: "DSC5103 Assignment 1"
subtitle: "Simulation and K-Nearest Neighbor Algorithm"
author: "Tong Wang"
date: "Aug 2016"
output:
  html_document:
    theme: yeti
    highlight: tango
  pdf_document:
    highlight: zenburn
---
<!--
comments must be put in an HTML comment form
-->

## NOTE:
This assignment is **due at 23:59 of Aug 25, Thursday**. You can work on this file directly and fill in your answers/code below. Please submit the output HTML file (name your file like G1Group02.html if you are from Group 02 of Section G1) onto IVLE/Files/Student Submission/Assignment1 folder.

Also, put the Section/Group and member info below.
```{r}
# Section G?
# Group ??
# Members: YOUR NAMES HERE
```



## Part I: Simulation
In this exercise, we will use simulation to illustrate the variability of statistics calculated from random samples. Suppose there is a **normal** population of size **10000**, with mean **100** and standard deviation **15**. Now we draw a sample from the population, of size **100** without replacement, we can calculate sample statistics such as mean and variance. If we further repeat the sampling process many times, say **200**, we will have 200 sets of similar sample statistics. Let's examine these sample statistics.

The necessary parameters are already set up as below.
```{r}
pop.size <- 10000
pop.mean <- 100
pop.sd <- 15

num.of.samples <- 200
sample.size <- 100
```

### Questions and Answers
1. Use random seed **1234** to conduct the simulation (i.e., simulate the population as specified, draw 200 samples, and calculate sample mean and variance for each sample, respectively), evaluate the mean and standard deviation of the sample statistics, and compare with their theoretical values. Draw histograms of the sample statistics. (1 Mark)

Answer: 

```{r}
# put your R code here inside the blocks

```



## Part II: K-Nearest Neighbor Algorithm

### Introduction
In this assignment, we are going to experiment the K-Nearest Neighbor (KNN) algorithm on a higher-dimensional dataset and experience the deterioration of prediction performance as the dimensionality grows.

The experiment is built on top of the 3rd-order polynomial model discussed in class (knn_demo.R), i.e.,
$$y = \beta_0 + \beta_1 * x + \beta_2 * x^2 + \beta_3 * x^3 + \epsilon, ~~ \epsilon \sim \text{N}(0, \sigma^2)$$
and we are going to introduce an extra 20-dimensional predictor $z$, which does NOT actually play a role in generating $y$. Yet, when in estimation, we do not know the fact and will use both $x$ and $z$ as predictors in the KNN algorithm.

### Generation of the high-dimensional dataset
We first simulate the 3rd-order polynomial datasets as we did in knn_demo.R. 

```{r}
library("ggplot2")

## population parameters
beta0 <- 1
beta1 <- -2
beta2 <- 6
beta3 <- -1
sigma <- 2

set.seed(7890)

## training data
x <- seq(0, 4.95, 0.05)
f_x <- beta0 + beta1 * x + beta2 * x^2 + beta3 * x^3
epsilon <- rnorm(n=100, mean=0, sd=sigma)
y <- f_x + epsilon

## test data
x.test <- seq(0, 5, 0.1)
f_x.test <- beta0 + beta1 * x.test + beta2 * x.test^2 + beta3 * x.test^3
epsilon.test <- rnorm(n=length(x.test), mean=0, sd=sigma)
y.test <- f_x.test + epsilon.test
```
The resulted training and test dataset have `r length(y)` and `r length(y.test)` data points, respectively.

Next, we need to generate $z$, the 20-dimensional predictors, of the same sizes. Each $z$ is a 20-dimensional multivariate normal random variable, with mean being $(0, 0, \ldots, 0)$ and identity covariance matrix (so that the 20 elements are independent standard normal random variables). The resulted $z$ is a 100*20 matrix, with each row being a data point with 20 dimensions.
```{r}
library("mvtnorm")  # package for multivariate normal distribution, INSTALL IT BEFORE RUNNING
z <- rmvnorm(n=100, mean=rep(0, 20))  # covariance matrix is identity matrix by default, no need to specify here
z.test <- rmvnorm(n=51, mean=rep(0, 20))
head(z)
```

Later, we will use $(x, z)$ to predict $y$. Let's first combine $x$ and $z$ into matrices, as required by function knn.reg().
```{r}
train.x <- cbind(x, z)
test.x <- cbind(x.test, z.test)
head(train.x)
```

### Questions

#### 1.	For a fixed $k=15$, fit a KNN model to predict $y$ with $(x, z)$, and measure the training and test MSE. (1 Mark)

Answer: 

```{r}
# put your R code here inside the blocks

```


#### 2.	With the same data, plot the training and test MSE of the KNN model against $k$, and find the optimal $k$ and the corresponding test MSE.  (1 Mark)

Answer: 

```{r}
# put your R code here inside the blocks

```

#### 3.	Based on the analysis above, compare the above model with $(x, z)$ being the predictors and the previous model with $x$ only (as in knn_demo.R). Briefly explain why.  (1 Mark)

Answer: 



#### 4.	We have seen that the test MSE is significantly worse than what we had without using predictor $z$ (in knn_demo.R). To better understand the impact of including irrelevant predictors in the KNN algorithm, let's try to include the 20 dimensions of $z$ one by one. So in each round $j$, we construct the predictors by combining $x$ and the first $j$ columns of $z$, then repeat the analysis in Question 2 and find the optimal $k$ and test MSE. At the end, plot the optimal MSE agaist $j$, and interpret the result.  (1 Mark)

Answer: 

```{r}
# put your R code here inside the blocks

```



## Session Info

```{r session-info}
print(sessionInfo(), locale=FALSE)
```
